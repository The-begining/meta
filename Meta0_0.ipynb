{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UoZrf4xmeD5l"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments, LlamaForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import subprocess\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rJZ1HY2keGCG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into './norec'...\n",
            "Updating files: 100% (43440/43440), done.\n"
          ]
        }
      ],
      "source": [
        "# Download dataset from GitHub\n",
        "repo_url = \"https://github.com/ltgoslo/norec.git\"\n",
        "dataset_path = \"./norec\"\n",
        "if not os.path.exists(dataset_path):\n",
        "    subprocess.run([\"git\", \"clone\", repo_url, dataset_path])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"NEBIUS_API_KEY\"] = \"eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExODExNzk3MzQ3MzE2MzA4NzM1NyIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NjEwMTEyNiwidXVpZCI6IjJiY2ZlYmU0LTkyZjMtNGY3NC1iZTFlLWYwNTMxNzI0ZWYyMiIsIm5hbWUiOiJNZXRhIiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMzFUMTQ6NDU6MjYrMDAwMCJ9.HWBvay7K9c0XcPwTRBk3tX86BlBi6mH6K-YC18ohdKM\"\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExODExNzk3MzQ3MzE2MzA4NzM1NyIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NjEwMTEyNiwidXVpZCI6IjJiY2ZlYmU0LTkyZjMtNGY3NC1iZTFlLWYwNTMxNzI0ZWYyMiIsIm5hbWUiOiJNZXRhIiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMzFUMTQ6NDU6MjYrMDAwMCJ9.HWBvay7K9c0XcPwTRBk3tX86BlBi6mH6K-YC18ohdKM\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.environ.get(\"NEBIUS_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "id": "9fxxpdkFeKC-",
        "outputId": "640a1990-6233-4056-ba9b-d9e3b7517bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-0228c4f66b5443e7ad3810d95fddea0e\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"Hello! It's nice to meet you. Is there something I can help you with or would you like to chat?\",\n",
            "        \"refusal\": null,\n",
            "        \"role\": \"assistant\",\n",
            "        \"audio\": null,\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": []\n",
            "      },\n",
            "      \"stop_reason\": null\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1738425303,\n",
            "  \"model\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"service_tier\": null,\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 25,\n",
            "    \"prompt_tokens\": 36,\n",
            "    \"total_tokens\": 61,\n",
            "    \"completion_tokens_details\": null,\n",
            "    \"prompt_tokens_details\": null\n",
            "  },\n",
            "  \"prompt_logprobs\": null\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI, OpenAIError\n",
        "\n",
        "# Get API Key\n",
        "api_key = os.environ.get(\"NEBIUS_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"NEBIUS_API_KEY not found! Set it as an environment variable.\")\n",
        "\n",
        "# Initialize Nebius AI client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Make a request\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "        temperature=0,\n",
        "        messages=[{\"role\": \"system\", \"content\": \"Hello, Nebius AI!\"}]\n",
        "    )\n",
        "\n",
        "    print(response.to_json())\n",
        "\n",
        "except OpenAIError as e:\n",
        "    print(\"üö® API Error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gNax0a0be4wv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_5GXumtkeVUl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Merged dataset saved as ./norec/norec_combined.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_folders = [os.path.join(dataset_path, \"data/train\"),\n",
        "                os.path.join(dataset_path, \"data/dev\"),\n",
        "                os.path.join(dataset_path, \"data/test\")]\n",
        "\n",
        "output_file = os.path.join(dataset_path, \"norec_combined.txt\")  # Save inside dataset_path\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for folder in data_folders:\n",
        "        if os.path.exists(folder):  # Ensure the folder exists\n",
        "            for filename in os.listdir(folder):\n",
        "                file_path = os.path.join(folder, filename)\n",
        "                if filename.endswith(\".txt\"):\n",
        "                    with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
        "                        outfile.write(infile.read() + \"\\n\")  # Add newline for separation\n",
        "\n",
        "print(f\"‚úÖ Merged dataset saved as {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Define the dataset file path\n",
        "#data_file = dataset_path   Change this to the correct path\n",
        "\n",
        "# Train a SentencePiece tokenizer\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=os.path.join(dataset_path, \"norec_combined.txt\"),\n",
        "    model_prefix=\"norec_tokenizer\",\n",
        "    vocab_size=32000,\n",
        "    model_type=\"bpe\",\n",
        "    character_coverage=0.9995,\n",
        "    max_sentence_length=4096\n",
        ")\n",
        "\n",
        "print(\"Tokenizer training complete! Model saved as 'norec_tokenizer.model'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "03Qh7SF3QCOB",
        "outputId": "a66abce3-1dce-4dc9-ccec-b9bd02e47396"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a632dd27f5b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlamaForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLlamaForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune for causal language modeling\n",
        "model_clm = LlamaForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "training_args_clm = TrainingArguments(\n",
        "    output_dir=\"./llama_norwegian_clm\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_dir=\"./logs_clm\",\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_clm = Trainer(\n",
        "    model=model_clm,\n",
        "    args=training_args_clm,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"]\n",
        ")\n",
        "\n",
        "trainer_clm.train()\n",
        "\n",
        "# Fine-tune for sequence classification\n",
        "model_cls = LlamaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: positive, neutral, negative\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=8, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_dataset[\"test\"], batch_size=8)\n",
        "\n",
        "training_args_cls = TrainingArguments(\n",
        "    output_dir=\"./llama_norwegian_cls\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5\n",
        ")\n",
        "\n",
        "trainer_cls = Trainer(\n",
        "    model=model_cls,\n",
        "    args=training_args_cls,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"]\n",
        ")\n",
        "\n",
        "trainer_cls.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BRQsrDAbM8_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
